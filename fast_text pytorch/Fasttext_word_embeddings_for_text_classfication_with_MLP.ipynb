{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zLQXumCdnyt-"
   },
   "source": [
    "# Using fasttext for text classification\n",
    "In this notebook we shall go over the use of fastText in pretrained word embeddings for converting text data into vector model. We have also loaded word embeddings into machine learning MLP algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yMpO4PpXgM8w"
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8CiPS--ugUOO",
    "outputId": "7373f741-ced2-4b42-da1e-c47e67659cbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-10-08 01:27:06--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 172.67.9.4, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 681808098 (650M) [application/zip]\n",
      "Saving to: ‘wiki-news-300d-1M.vec.zip’\n",
      "\n",
      "wiki-news-300d-1M.v 100%[===================>] 650.22M  21.5MB/s    in 31s     \n",
      "\n",
      "2021-10-08 01:27:38 (20.9 MB/s) - ‘wiki-news-300d-1M.vec.zip’ saved [681808098/681808098]\n",
      "\n",
      "Archive:  wiki-news-300d-1M.vec.zip\n",
      "  inflating: wiki-news-300d-1M.vec   \n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
    "!unzip wiki-news-300d-1M.vec.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CHWLN9IxgwpX",
    "outputId": "29b724a5-281a-4b48-f5d3-c353038c8045"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('desks', 0.7923153638839722), ('Desk', 0.6869951486587524), ('desk.', 0.6602819561958313), ('desk-', 0.6187258958816528), ('credenza', 0.5955315828323364), ('roll-top', 0.5875717401504517), ('rolltop', 0.5837830305099487), ('bookshelf', 0.5758029222488403), ('Desks', 0.5755287408828735), ('sofa', 0.5617446899414062)]\n"
     ]
    }
   ],
   "source": [
    "model = KeyedVectors.load_word2vec_format(\"/content/wiki-news-300d-1M.vec\")\n",
    "print(model.most_similar('desk'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Qy3jvOMEhbAz"
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "for word in model.vocab:\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "viz9eIjujWl8",
    "outputId": "a3547cde-2af9-40ba-f761-f96994c498dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector components of a word: [ 1.0730e-01  8.9000e-03  6.0000e-04  5.5000e-03 -6.4600e-02 -6.0000e-02\n",
      "  4.5000e-02 -1.3300e-02 -3.5700e-02  4.3000e-02 -3.5600e-02 -3.2000e-03\n",
      "  7.3000e-03 -1.0000e-04  2.5800e-02 -1.6600e-02  7.5000e-03  6.8600e-02\n",
      "  3.9200e-02  7.5300e-02  1.1500e-02 -8.7000e-03  4.2100e-02  2.6500e-02\n",
      " -6.0100e-02  2.4200e-01  1.9900e-02 -7.3900e-02 -3.1000e-03 -2.6300e-02\n",
      " -6.2000e-03  1.6800e-02 -3.5700e-02 -2.4900e-02  1.9000e-02 -1.8400e-02\n",
      " -5.3700e-02  1.4200e-01  6.0000e-02  2.2600e-02 -3.8000e-03 -6.7500e-02\n",
      " -3.6000e-03 -8.0000e-03  5.7000e-02  2.0800e-02  2.2300e-02 -2.5600e-02\n",
      " -1.5300e-02  2.2000e-03 -4.8200e-02  1.3100e-02 -6.0160e-01 -8.8000e-03\n",
      "  1.0600e-02  2.2900e-02  3.3600e-02  7.1000e-03  8.8700e-02  2.3700e-02\n",
      " -2.9000e-02 -4.0500e-02 -1.2500e-02  1.4700e-02  4.7500e-02  6.4700e-02\n",
      "  4.7400e-02  1.9900e-02  4.0800e-02  3.2200e-02  3.6000e-03  3.5000e-02\n",
      " -7.2300e-02 -3.0500e-02  1.8400e-02 -2.6000e-03  2.4000e-02 -1.6000e-02\n",
      " -3.0800e-02  4.3400e-02  1.4700e-02 -4.5700e-02 -2.6700e-02 -1.7030e-01\n",
      " -9.9000e-03  4.1700e-02  2.3500e-02 -2.6000e-02 -1.5190e-01 -1.1600e-02\n",
      " -3.0600e-02 -4.1300e-02  3.3000e-02  7.2300e-02  3.6500e-02 -1.0000e-04\n",
      "  4.2000e-03  3.4600e-02  2.7700e-02 -3.0500e-02  7.8400e-02 -4.0400e-02\n",
      "  1.8700e-02 -2.2500e-02 -2.0600e-02 -1.7900e-02 -2.4280e-01  6.6900e-02\n",
      "  5.2300e-02  5.2700e-02  1.4900e-02 -7.0800e-02 -9.8700e-02  2.6300e-02\n",
      " -6.1100e-02  3.0200e-02  2.1600e-02  3.1300e-02 -1.4000e-02 -2.4950e-01\n",
      " -3.4600e-02 -4.8000e-02  2.5000e-02  2.1300e-01 -3.3000e-02 -1.5530e-01\n",
      " -2.9200e-02 -3.4600e-02  1.0740e-01  1.0000e-03 -1.1700e-02 -5.7000e-03\n",
      " -1.2800e-01 -3.8000e-03  1.3000e-02 -1.1570e-01 -1.0800e-02  2.7500e-02\n",
      "  1.5800e-02 -1.6900e-02  7.0000e-03  2.4700e-02  5.1000e-02  1.0292e+00\n",
      " -2.8300e-02 -3.1000e-02 -2.6000e-03 -3.4300e-02  5.7800e-02  4.4400e-02\n",
      "  8.1200e-02 -2.1100e-02 -8.7200e-02  1.6900e-02  4.9900e-02  4.8500e-02\n",
      "  2.2700e-02 -3.2300e-02 -3.5000e-03  4.3500e-02 -2.7500e-02  1.5400e-02\n",
      "  1.3500e-02 -4.8400e-02 -6.9900e-02 -5.0200e-02  2.7450e-01 -3.0000e-04\n",
      " -3.7100e-02  5.1700e-02 -9.0800e-02  1.3000e-03  3.6000e-02  2.8000e-02\n",
      "  8.3900e-02  9.8000e-02 -4.9000e-02 -2.4230e-01 -1.4200e-02  2.4000e-03\n",
      " -2.0700e-02  1.2000e-03  8.8000e-03 -1.4300e-02 -1.9700e-02  5.1500e-02\n",
      " -8.5000e-03  2.5700e-02  2.1540e-01  3.0100e-02  2.1100e-02  5.3000e-02\n",
      " -5.0000e-04  1.7700e-02  1.6000e-03 -5.3000e-03 -1.6200e-02 -2.2300e-02\n",
      " -1.8620e-01  3.9800e-02  6.5800e-02 -9.6200e-02 -7.6000e-03 -7.5000e-03\n",
      " -3.4200e-02 -2.6500e-02  4.2000e-02  5.2200e-02 -2.6600e-02  2.0100e-02\n",
      " -1.3310e-01 -3.6700e-02  3.5100e-02  5.1800e-02 -8.7000e-03  5.9900e-02\n",
      " -1.0860e-01 -1.8800e-02  4.8100e-02  1.0500e-02 -6.0000e-03  1.5100e-02\n",
      " -3.1000e-03  7.7000e-03 -2.7600e-02 -3.7300e-02 -2.0300e-02  4.7200e-02\n",
      "  2.4600e-02  1.4400e-01  5.4200e-02 -2.2500e-02  2.4950e-01  1.6170e-01\n",
      "  3.8000e-03  1.1190e-01 -2.3000e-02 -7.8500e-02  2.5000e-02 -6.1600e-02\n",
      " -4.8500e-02  2.2500e-02  2.8100e-02  4.1000e-03  1.1200e-02  1.7200e-02\n",
      "  2.9100e-02 -2.8200e-02  2.6000e-03  4.0550e-01  3.9200e-02  8.8000e-03\n",
      "  2.2800e-02  2.9900e-02  1.1950e-01  5.4500e-02 -2.0000e-03  2.0000e-03\n",
      "  4.9000e-02  1.4500e-02 -8.6000e-03  9.8000e-03 -2.3600e-02  1.7100e-02\n",
      " -7.6500e-02 -4.0000e-02  1.2800e-02  1.1000e-03  4.2000e-03  2.4400e-02\n",
      "  7.5000e-03  2.0000e-02  2.0100e-02  1.9600e-02 -3.7700e-02 -4.3200e-02\n",
      " -7.3000e-03 -2.1000e-03  1.8300e-02  7.6000e-03  1.8050e-01 -5.5100e-02\n",
      "  7.5000e-03 -5.1600e-02  4.2000e-02 -6.8000e-03 -7.1100e-02 -1.4080e-01\n",
      "  5.0400e-02  2.7600e-02  4.7000e-02  3.2300e-02 -2.1900e-02  1.0000e-03\n",
      "  8.9000e-03  2.7600e-02  1.8600e-02  5.0000e-03  1.1730e-01 -4.0000e-02]\n"
     ]
    }
   ],
   "source": [
    "print(\"Vector components of a word: {}\". format(model[words[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pp86rXXajiso"
   },
   "source": [
    "# The Problem\n",
    "We will use fastText word embeddings for text classifciation of sentences, we will use the sklearn MLP. The sentences are prepared and inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9wWwB_5jjfyU"
   },
   "outputs": [],
   "source": [
    "sentences = [['this', 'is', 'the', 'good', 'machine', 'learning', 'book'],\n",
    "            ['this', 'is',  'another', 'machine', 'learning', 'book'],\n",
    "            ['one', 'more', 'new', 'book'],\n",
    "         \n",
    "          ['this', 'is', 'about', 'machine', 'learning', 'post'],\n",
    "          ['orange', 'juice', 'is', 'the', 'liquid', 'extract', 'of', 'fruit'],\n",
    "          ['orange', 'juice', 'comes', 'in', 'several', 'different', 'varieties'],\n",
    "          ['this', 'is', 'the', 'last', 'machine', 'learning', 'book'],\n",
    "          ['orange', 'juice', 'comes', 'in', 'several', 'different', 'packages'],\n",
    "          ['orange', 'juice', 'is', 'liquid', 'extract', 'from', 'fruit', 'on', 'orange', 'tree']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kn9y6evsj_oT"
   },
   "source": [
    "The problem of classifying the words can be visualized by the following flowchart <img src=\"https://bit.ly/3oCtx5U)\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGGK-QD5ksYK"
   },
   "source": [
    "# Data preparation\n",
    "we have a task of preparing the data into digital output. The aim will be to get word embeddings and avergae all words in the sentences. The resulting vector sentence representations are saved to array V. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Jmbbl9Fuj6u0"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "def sent_vectorizer(sent, model):\n",
    "    sent_vec = []\n",
    "    numw = 0\n",
    "    for w in sent:\n",
    "        try:\n",
    "            if numw  == 0:\n",
    "                sent_vec = model[w]\n",
    "            else:\n",
    "                sent_vec = np.add(sent_vec, model[w])\n",
    "            numw += 1\n",
    "        except:\n",
    "            pass\n",
    "    return np.asarray(sent_vec) / numw \n",
    "\n",
    "V = []\n",
    "for sentence in sentences:\n",
    "    V.append(sent_vectorizer(sentence, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fXHHY8ylqjj"
   },
   "source": [
    "After converting the text to vectors we can then divide the data into training and testing datasets and attach class albels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "C0sfkDKVlis9"
   },
   "outputs": [],
   "source": [
    "X_train = V[0:6]\n",
    "X_test = V[6:9]\n",
    "\n",
    "Y_train = [0, 0, 0, 0,1,1]\n",
    "Y_test = [0, 1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5LSfCuoymAfO"
   },
   "source": [
    "# Text classification \n",
    "It's time to load the MLP classifer for the text classifcation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "FLLbgcCel9v4"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zqf_ofUimZiS",
    "outputId": "5f3ec1d3-4000-4578-8466-a72adac41c1e",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.7, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=400,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = MLPClassifier(alpha = 0.7, max_iter=400)\n",
    "classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f2DesroEmmEM",
    "outputId": "6ff19a70-cae3-4eb7-c215-d3eeef118504"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.76104308 0.23895692]\n",
      " [0.49024427 0.50975573]\n",
      " [0.44699475 0.55300525]]\n",
      "[0 1 1]\n",
      "  classifier  train_score  test_score\n",
      "0          0          0.0         0.0\n",
      "1        MLP          1.0         1.0\n"
     ]
    }
   ],
   "source": [
    "df_results = pd.DataFrame(data=np.zeros(shape=(1,3)), columns=['classifier', 'train_score', 'test_score'])\n",
    "train_score = classifier.score(X_train, Y_train)\n",
    "test_score = classifier.score(X_test, Y_test)\n",
    "\n",
    "print(classifier.predict_proba(X_test))\n",
    "print(classifier.predict(X_test))\n",
    "\n",
    "df_results.loc[1,'classifier'] = 'MLP'\n",
    "df_results.loc[1,'train_score'] = train_score\n",
    "df_results.loc[1,'test_score'] = test_score\n",
    "\n",
    "print(df_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k57GppM-nMqi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Fasttext_word_embeddings_for_text_classfication_with_MLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
